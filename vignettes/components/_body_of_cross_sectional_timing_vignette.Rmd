---
output:
  pdf_document: default
  html_document: default
---

# Introduction

```{r child = here::here("vignettes/components/_contents.Rmd"), ref = "contents"}

```

Thus, a user can easily simulate the use of MTS with their own value stream by
creating homologs of the following three files:

-  `r parent_file`
-  `r customization_file`
-  `r basename(activity_file)`

```{r child = here::here("vignettes/components/_theoretical_background.Rmd"), ref = "theory"}

```


# Purpose

Management of a value stream requires adequate estimates of the resources used
to create value, with time-on-task being a direct measure of the primary
resource^[Employee effort, as measured by time, is the most expensive resource.
In addition, creating a work environment that encourages retention of employees
requires time management through appropriate prioritization so that employees
are maximally productive and fulfilled.] creating value. The need to
obtain quality estimates of time spent on the various tasks in a value stream
is the motivation to provide these tools and to advocate for the use of MTS (a
cross-sectional time-on-task estimation method) for the great majority of
complex labor intensive value streams.

In the following example, estimates are obtained of how much time is spent on
specific activities by various actors^[
`r get_and_or_list(unique(activities$actor_type))`] responsible for the success of 
the `r value_stream_descriptor`. 
Time-on-task data provide a fundamental metric used to identify those areas
where improvement opportunities exist.

Detailed planning regarding questions to be addressed is required prior to 
creating the list of activities to be timed. 
(See [Selection of Activities to be Timed].)

A benefit of time-on-task studies is that they provide accurate and precise^[
The simulation method demonstrated later in this document provides a robust
estimate of expected precision.]
data regarding how
time is spent and how much of it directly contributes to our primary goals. A
second benefit is that these data allow each type of actor to advocate for
modifying the way their time is being used to improve their work environment
and productivity.

# Comparison of Some Time Analysis Approaches

## Duration Between Milestones

Studies that measure duration between value stream milestones have the following
weaknesses.

-   Good estimates of duration between value stream milestones require multiple
    observations per milestone interval and observing the entire value stream
    (entire duration) multiple times.
    -  This means little information is obtained until multiple full passes 
       through the value stream have occurred. Thus, no data driven improvements
       can be implemented while waiting for multiple complete 
       passes to occur. This represent an unacceptable lost-opportunity-cost 
       for those value streams taking any approciable amount of time.
-   Measures of duration between milestones provide no insight into
    percent effort expended to create value within the duration.
    -  Thus, once you have estimates of duration between milestones, you 
       remain blind to how the resources were used within that duration.
-   Milestones do not provide the granularity of specific tasks.
    -  Milestones, by their nature, are reached through multiple actions of 
       a few to many different types. The percent of effort and the pattern
       of effort in those actions remains unknown.
-   Milestones impose a linear view of work while the value stream may contain 
    several important feedback loops.
<!-- TODO create actions that indicate what milestone they exist in or what
feedback loop they are part of -->

## Time Logging

Longitudinal time-on-task estimation methods that call for representative 
actors within each group to log time on tasks during their work day have the
weaknesses listed below.

-   Failure rate in logging activities can be high.
-   Recall of the amount of time spent is often wrong, resulting in an
    artificially inflated variance.
-   Preconceptions of how time should be spent biases recalled durations.
-   Keeping, collecting, and analyzing logs requires significant administrative
    overhead.
-   The biases described in earlier points makes interpretation of results
    difficult.
-   The longitudinal collection of data requires a long data collection
    phase before any analyses can begin.
-   Variance among different projects is anticipated to be high, which
    further complicates analysis since all projects have to have completed
    all phases of the projects under analysis.

## Momentary Time Sampling

The MTS design presented^[The MTS design assumes that we have the ability to 
send a question to selected individuals at selected times and collect 
responses.] and simulated below has the following advantages.

-   The _failure to respond rate_^[MTS requires periodic momentary sampling by
    requesting the actor to indicate what she was doing when the request 
    arrived. (In those instances where the actor is not capable of responding, 
    such as in animal behavior studies, a request for input is made to the 
    data collecting observer of the actor.) 
    _Failure-to-respond_ can occur because the actor is away from work or 
    engaged in an activity where the request is not received or cannot be 
    responded to. In some cases, feedback by the actor to the studiy's point 
    of contact is needed to identify those activities that are significant 
    contributors to the _failure-to-respond rate_]
    has far less impact since failure to respond
    simply initiates another response request.
-   Participants do not need to recall time spent.
-   Administrative overhead is greatly reduced.
-   Bias is minimized and is effectively zero given accurate responses by 
    actors.
-   Data can be analyzed at any time.^[Assuming multiple value streams of the 
    same type are in process and are at different phase of the value stream, 
    estimates of time-on-task for all phases of the value stream are almost 
    immediately available and become more precies with each subsequent sample.]
-   Variance among projects could be assessed and more accurately measured if
    desired.^[While project differences can be ignored during intitial timing 
    studies, project metadata will be a rich source of meaningful segmentation 
    as sufficient data become available.]

# Example Measurement Protocol

Detailed planning regarding questions to be addressed and how and when those
questions are presented to participants is required for the successful use of
any time-on-task measurement protocol.

## Selection of Activities to be Timed

Creation of the activity list is a critical step. The list is to include 
everything that should be measured and leave out
those that should not be measured. Unambiguous definitions of the boundaries of
the value stream and of the selected activities is essential.

One or more individuals familiar with all potential workday activities should
make an initial trial list. This list should then be submitted to participants
for further refinement. Having the participants help create and refine the list
is critical to assuring good coverage of possible activities, to reinforce
the nature of this as a team effort to gain insight for workplace enhancement,
and to prevent interpretation of the process as an imposed activity designed 
to identify poor work habits. Further,
it is preferable that the management groups of the primary actors participate 
in an analogous time-on-task study during the same time period. The results
and analyses should be equally transparent and published.

The goal is to discover how the actors' work
environment can be improved. Thus, measuring time spent on activities not
directly contributing to the value stream(s) being studied is critical to
enhanced understanding of the work environment in which the value stream
work is done.
Examples, include administrative activities, non-value stream related work,
general training, non-mission critical meetings, and
dealing with work blocking technical issues.

Some actors will work on more than one value stream and all will have
additional responsibilities specific to their work environment. Thus, some
activities recorded are intentionally defined as various unrelated activities.
The activity list could include __Personal__ to assure participants that
management supports a healthy work-life balance.  This is an example where the
inclusion of activities not directly related to the value stream under study may
be appropriate. These activities could include references to practices
encouraged by management such as ongoing education, exercise breaks, and
helping co-workers. It is critical that these aspects of the work landscape are
clearly described so that any negative stigma associated with non value stream
related activities are accurately captured.

Finding that the activity equivalent to __Other__ in the simulation below
represents more time than anticipated may be a reason to collect additional
information to identify sub components.

An important advantage of MTS is that later refinement of the activity list to 
more accurately capture desired information can be performed at any time 
without
loss of information.^[This does require version control of the
activity list recording the version ID being 
used with the every record.]

## Collecting the Timing Data

The method used to collect the timing data should have minimal impact on the
actor's day.  One ideal method would be to have a modal box appear on an actor's
computer monitor and request a minimal amount of input so that the modal box
can be dismissed in less than 10 seconds where fewer seconds are desirable.

A webpage could be used by all participants to indicate a willingness to
participate, their current availability, and provide any basic metadata needed
to ensure the correct activities list is provided and the data collected is
appropriately aggregated for analysis.

This webpage could interact via a web API to a background application that
contained the various activity lists being used, an algorithm for selecting
when actors are prompted for a current activity response, and a method to
accept the response data and store it in a database that can later be accessed
for data analysis.

Group chat tools, such as __Slack__^[Slack is an online communication platform
for teams] may have APIs to provide similar services.^[See,
https://api.slack.com/block-kit/building for a possible Slack approach.]

It is recommended that uniformly pseudo-random times throughout the work period
(typically day, week, and month) would be used so that systematic work
schedules will not bias the results.

## Data Analysis and Followup

Planning how much detail to collect and how the data will be analyzed should
occur prior to any data being collected.  This clarity of purpose will promote
rapid recognition of unexpected data trends that may prompt revision of the
activity lists or collection methodology.

The primary analysis for each value stream should include calculating the time
estimates for each
activity.  $$time_{activity_i} = m_i * time\_units / \sum_{i=1}^{i=n} m_i$$
where $i$ is the index of the activity of interest, $time\_units$ is the total
number of units of the desired unit of measure within the time period of
interest, such as 560 minutes in a typical 8 hour work day, $m_i$ is the number
of times activity $i$ was observed, $n$ is the number of distinct activities.

See the [description of 
Table \@ref(tab:small-sample-in-minutes)] 
for an example where the calculation of
minutes per activity in one 8 hour day is illustrated.

Prior to the collection of data, the various questions to be addressed should be
articulated along with a description of how the provided activity durations
will address those questions. In addition to questions to be addressed,
expected relationships among various subset of activities should be recorded
for later reference.

This is a critical part of the analysis as the full appreciation of our
ignorance prior to collecting the data cannot be fully realized without this
record. Without this concrete evidence, we tend to dismiss our ignorance as
trivial and of no practical importance in planning our reactions to our
findings.  This clearer understanding of what we do not know will better guide
our next actions.

Plan on using at least two sampling periods as it is expected that the activity
granularity will likely need adjusting based on initial results. Also, followup
studies will be critical as behavioral and management changes should be
assessed. This low impact sampling design makes this possible.

# Simulation

The remainder of this document presents a
simulation of an example data collection using a realistic design that 
provides sufficient detail to
assess expected accuracy and precision of the resulting MTS estimates
for the `r length(activities$activity)` activities.

## Design

The example has a set of representative activities for `r actor_type`s.^[ The
software has the ability to examine multiple job types, job specific
activity lists, and corresponding expected frequencies.]
It simulates asking
a set of `r actor_type`s to indicate which of the activities listed on the
questionnaire they were doing at the time the question appeared on their
screen. 
Once the `r actor_type` makes a selection, that selection is returned as a
result to the collection software, which accumulates the responses for later
analysis.

In the simulation model, these queries and responses can all be processed in
less than a second but, as described below, this simulation is constructed so
that each `r actor_type` responds 
<!--`r numbers2words(times_per_month)`-->
`r get_and_or_list(numbers2words(c(times_per_month, 2 * times_per_month, 4 * times_per_month)))`
times respectively
during each of the three experiments illustrated below.

The manner in which the questions are presented and the timing of when the
questions are presented are critical aspects of the study design but are not
part of the simulation.

## Simulation Activity Frequencies

The `r actor_type`s' activities are programmed to occur with the following
frequencies as specified in the CSV file.

```{r activities, echo = FALSE}
activities <- activities[activities$actor_type == actor_type, ]
caption <- stri_c("List of possible activities for the ",
                  stri_replace_all_fixed(actor_type, pattern = "_",
                                         replacement = "\\_"),
                  " with assigned frequencies to be used in simulations.")
activities[, c("activity", "freq")] %>%
  kbl(digits = 2, col.names = c("Activity", "Frequency"),
      caption = caption, row.names = FALSE) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_material(c("striped", "hover"))

```

## Data Collection Example

The data have a simple structure. A single observation will have the following
fields at a minimum.

-  Actor's Id, which is typically known because the request for 
   information is often intentionally balanced among all actors which requeres 
   tracking actor Id with the response.
-  Project Id, which will be mapped to further metadata needed for complete 
   analysis.
-  Actor type(_`r actor_type`_, which may be only one of multiple roles an 
   actor may have. The various roles are metadata that are critical for 
   meaningful analysis.
-  Activity name.
-  Date and time stamp.

For clarity, Table \@ref(tab:three-samples) lists the results from sampling three 
times from a pool of `r actor_type`s.

```{r three-samples}
size <- 3
three_samples <- make_activity_observation(
  activities, size = size, activities$freq)

dateTime <- paste0("2023-09-16 12:", c("12:07", "23:18", "54:42"), " CDT")
three_samples_df <-
  data.frame(aId = paste0("A", sample(1:10, size, replace = TRUE)),
             pId = paste0("P", sample(1:10, size, replace = TRUE)),
             Actor_Roll = actor_type,
             Activity = three_samples,
             DateTime = dateTime, stringsAsFactors = FALSE)
caption <- "\\label{tab:three-samples}Example of three rows of MTS data."
three_samples_df %>%
  kbl(col.names = c("Actor ID", "Project ID", "Actor Type", "Activity",
                    "Date Time Stamp"),
      caption = caption, row.names = FALSE) %>%
  column_spec(1, width = "4em") %>%
  column_spec(2, width = "5em") %>%
  column_spec(3, width = "10em") %>%
  column_spec(4, width = "18em") %>%
  column_spec(5, width = "11em") %>%
  kable_styling(latex_options = "HOLD_position",
                font_size = 8) %>%
  kable_material(c("striped", "hover"))

```


In the single iteration of the simulation below, we observe how precise the
estimates are if we include `r n_actors` `r actor_type`s sampled just 
`r times_per_month` times a month for each of `r n_months` months.

Since the algorithm is not actually modeling individual `r actor_type`s, the order
of sampling has no impact on results, which means we simply take 
`r n_actors * times_per_month * n_months`
(`r n_actors` * `r times_per_month` * `r n_months`)
samples per result set and compare each to the expected values.

```{r small-sample,}
small_sample <- make_activity_observation(
  activities, size = n_actors * times_per_month * n_months, activities$freq)
small_sample_counts <- table(small_sample)
sample_size <- sum(small_sample_counts)
small_sample_duration <- small_sample_counts / sample_size

small_sample_duration <-
  make_complete_sample_durations(small_sample_duration, activities)

activities <- activities[order(activities$activity), ]

caption <- stri_c("List of activities, the frequency in which they were ",
                  "observed, the simulated frequency of the activity ",
                  "(expected), and the absolute difference between the ",
                  "observed and expected frequencies.")

delta <- abs(small_sample_duration$duration - activities$freq)
small_sample_duration %>%
  cbind(data.frame(prob = activities$freq, delta = delta,
                   percent = 100 * delta / activities$freq)) %>%
  kbl(digits = 3,
      col.names = c("Description", "Observed", "Expected", "Delta",
                    "Delta as % of Expected"),
      caption = caption) %>%
  column_spec(1, width = "10em") %>%
  column_spec(2, width = "5em") %>%
  column_spec(3, width = "5em") %>%
  column_spec(4, width = "5em") %>%
  column_spec(5, width = "11em") %>%
  kable_styling(latex_options = "HOLD_position",
                font_size = 12) %>%
kable_styling(latex_options = "HOLD_position") %>%
  add_header_above(c(" " = 1, "Duration" = 4)) %>%
  kable_material(c("striped", "hover"))

```

It is sometimes helpful to remap these percentages into minutes per 8 hour day,
which can be done by allocating the `r n_actors * times_per_month * n_months`
observations into 560 minutes.

```{r small-sample-in-minutes}
minutes_per_activity <- small_sample_duration$duration * 560
minutes_per_activity <-
  data.frame(description = small_sample_duration$description,
             minutes = as.numeric(minutes_per_activity))
caption <- stri_c("List of activities and the estimated number of minutes ",
                  "spent on the activity if represented in a single day.")
minutes_per_activity %>%
  kbl(digits = 1, col.names = c("Description", "Minutes"),
      caption = caption) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_material(c("striped", "hover"))

```

I have run this experiment several times and usually the estimates are shown to
be very close as in seen in the _Delta_ column (usually within 1 percent of 
the expected value).
However, it is more instructive to repeat this experiment many times to
learn what precision we can expect.

## Results

Figure \@ref(fig:hist-1X) shows a histogram plot of the results of repeating
the simulation of `r n_actors` 
`r stri_replace_all_fixed(actor_type, pattern = "_", replacement = " ")`s
providing `r times_per_month` responses in each of the 
`r n_months` months of a year `r iterations` times.  Similarly, Figures
\@ref(fig:hist-2X) and \@ref(fig:hist-4X) show histogram plots of similar 
simulations with each `r actor_type` being sampled
`r round(times_per_month * 2, 0)` and `r round(times_per_month * 4, 0)`
time per month respectively instead of `r round(times_per_month, 0)` times.

The variance of each estimate ($\sigma^2 = n * p (1 - p)$) is linear 
with $n$ (number of observations). This translates to a simple method of 
predicting the improvement in estimates from MTS. 
That is, the size of the expected 
percent error can by reduced by a factor of 0.5 each time the number of samples
is quadrupled.


```{r}
this_times_per_month <- times_per_month
sim_delta_freq <- sim_sample_duration(
  activities,
  size = n_actors * this_times_per_month * n_months,
  iterations = iterations)
gt_1_percent <- sum(sim_delta_freq > 0.01) / length(sim_delta_freq)
gt_2_percent <- sum(sim_delta_freq > 0.02) / length(sim_delta_freq)
gt_3_percent <- sum(sim_delta_freq > 0.03) / length(sim_delta_freq)

hist_1X_cap <-
  stri_c("Simulation of ",
         format(iterations, decimal.mark = ".",
                big.mark = ","),
         " iterations with ",
         n_actors, " ",
         stri_replace_all_fixed(actor_type, pattern = "_", replacement = " "),
         "s sampled ", this_times_per_month,
         " times per month for one year found ",
         100 * gt_1_percent, "% of durations \nwere greater than ",
         "1 percent away from the ",
         "expected value and ",
         100 * gt_2_percent, "% of durations \nwere greater than ",
         "2 percent away from the ",
         "expected value and ", 100 * gt_3_percent,
         "% of \ndurations were greater than 3 ",
         "percent away from the expected value \nwith the largest ",
         "absolute delta value being ", round(max(sim_delta_freq), 3),
         ".")

```


```{r hist-1X, fig.cap=hist_1X_cap, fig.pos="H"}
data.frame(freq = sim_delta_freq) %>%
  ggplot(aes(x = freq)) +
  geom_histogram(bins = 30) +
  ggtitle(stri_c(n_actors, " ",
         stri_replace_all_fixed(actor_type, pattern = "_", replacement = " "),
         "s sampled ", this_times_per_month,
                 " times per month for one year")) +
  xlab("Absolute Value of (Expected - Observed) Frequency") +
  ylab("Count")

```
```{r}
this_times_per_month <- 2 * times_per_month
sim_delta_freq <-
  sim_sample_duration(activities,
                      size = n_actors * this_times_per_month * n_months,
                      iterations = iterations)
gt_1_percent <- sum(sim_delta_freq > 0.01) / length(sim_delta_freq)
gt_2_percent <- sum(sim_delta_freq > 0.02) / length(sim_delta_freq)
gt_3_percent <- sum(sim_delta_freq > 0.03) / length(sim_delta_freq)

hist_2X_cap <-
  stri_c("Simulation of ",
         format(iterations, decimal.mark = ".",
                big.mark = ","),
         " iterations with ",
         n_actors, " ",
         stri_replace_all_fixed(actor_type, pattern = "_", replacement = " "),
         "s sampled ", this_times_per_month,
         " times per month for one year found ",
         100 * gt_1_percent, "% of durations \nwere greater than ",
         "1 percent away from the ",
         "expected value and ",
         100 * gt_2_percent, "% of \ndurations were greater than ",
         "2 percent away from the ",
         "expected value and \n", 100 * gt_3_percent,
         "% of durations were greater than 3 ",
         "percent away from the expected \nvalue with the largest ",
         "absolute delta value being ", round(max(sim_delta_freq), 3),
         ".")

```
```{r hist-2X, fig.cap=hist_2X_cap, fig.pos="H"}
data.frame(freq = sim_delta_freq) %>%
  ggplot(aes(x = freq)) +
  geom_histogram(bins = 30) +
  ggtitle(paste0(round(n_actors, 0), " ",
         stri_replace_all_fixed(actor_type, pattern = "_", replacement = " "),
         "s sampled ", this_times_per_month,
                 " times per month for one year.")) +
  xlab("Absolute Value of (Expected - Observed) Frequency") +
  ylab("Count")

```

```{r}
this_times_per_month <- 4 * times_per_month
sim_delta_freq <-
  sim_sample_duration(activities,
                      size = n_actors * this_times_per_month * n_months,
                      iterations = iterations)
gt_1_percent <- sum(sim_delta_freq > 0.01) / length(sim_delta_freq)
gt_2_percent <- sum(sim_delta_freq > 0.02) / length(sim_delta_freq)
gt_3_percent <- sum(sim_delta_freq > 0.03) / length(sim_delta_freq)

hist_4X_cap <-
  stri_c("Simulation of ",
         format(iterations, decimal.mark = ".",
                big.mark = ","),
         " iterations with ",
         n_actors, " ",
         stri_replace_all_fixed(actor_type, pattern = "_", replacement = " "),
         "s sampled ", this_times_per_month,
         " times per month for one year found ",
         100 * gt_1_percent, "% of durations \nwere greater than ",
         "1 percent away from the ",
         "expected value and ",
         100 * gt_2_percent, "% of \ndurations were greater than ",
         "2 percent away from the ",
         "expected value and \n", 100 * gt_3_percent,
         "% of durations were greater than 3 ",
         "percent away from the expected \nvalue with the largest ",
         "absolute delta value being ", round(max(sim_delta_freq), 3),
         ".")

```
```{r hist-4X, fig.cap=hist_4X_cap, fig.pos="H"}
data.frame(freq = sim_delta_freq) %>%
  ggplot(aes(x = freq)) +
  geom_histogram(bins = 30) +
  ggtitle(paste0(round(n_actors, 0), " ",
         stri_replace_all_fixed(actor_type, pattern = "_", replacement = " "),
         "s sampled ", this_times_per_month,
                 " times per month for one year.")) +
  xlab("Absolute Value of (Expected - Observed) Frequency") +
  ylab("Count")

```

`r clearpage()`

## Conclusions

Prior experiments with this cross-sectional activity time estimation have shown
that about 250 samples will provide time estimates for up to 20 activities that
are all accurate and precise within $\pm2$ percent, which should be precise
enough for most management activities. As this estimation method provides
unbiased estimates for activities with more than momentary duration, no amount 
of sampling using actor logs can provide similar accuracy and precision because 
of inherent cognitive bias introduced by the actors' own expectations.


## References

<div id="refs"></div>



## Appendix

See the R script file sourced by the `customization` code chunk of this 
vignette to see how this document was customizeed.
To examine results you can expect from your own time-on-task activity analysis,
you will need to provide the CSV file^[Comparable to
colony_management_defined_activities.csv] with your own actors^[Actors are used
as variable names so must follow rules for naming variables in R and they are 
used in LaTex so underscores may not be allowed. They must
start with a letter, contain only letters, numerals, ".", and "_".],
activities, and activity descriptions in addition to providing your definitions
for the setup chunk definitions indicated below.

```{r key-value-pairs, eval = FALSE, echo = TRUE}
activity_file <-
  system.file("extdata",
              "colony_management_defined_activities.csv",
              package = "mtsr", lib.loc = NULL,
              mustWork = FALSE)
value_stream_descriptor <- stri_c("husbandry and veterinary care of ",
                                  "colony animals")
actor_type <- "animal_caretaker" # can be any value in the actor column of the
                            # activity file

# The product of n_actors, times_per_month, and n_months is the size.
# The size should be about 250 for about 20 activities to get the precision
# illustrated herein
n_actors <- 20
times_per_month <- 5
n_months <- 12

iterations <- 5000          # 5000 seems more than adequate for stable results
```
